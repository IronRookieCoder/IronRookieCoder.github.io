### **202X AI 全栈工程师实战能力认证考试**

**考生须知：**
*   **考试时间：** 4 小时
*   **总题量：** 15 题（五大方向各 3 题）
*   **评分维度：** 代码运行成功率 (30%)、核心逻辑正确性 (40%)、性能与鲁棒性优化 (30%)。
*   **提交物：** 关键代码文件（.py/.cu）、运行日志（Logs）、最终效果截图或数据文件。

---

### **第一方向：大模型 Agent (智能体构建)**
**核心要求：** 使用 **Qwen-7B (Instruct/Chat)** 模型；通过架构设计解决 LLM 的概率性幻觉，确保在数学与逻辑任务上达到 **100% 正确率**。

**题目 1：零误差数学计算器 (Code Interpreter)**
*   **项目背景：** 传统的思维链（CoT）在多位数乘除或微积分中仍有错误率。本项目要求利用 Qwen-7B 强大的工具调用能力，构建一个绝不出错的计算 Agent。
*   **基础环境：** **Qwen-7B-Instruct**（已加载），Python 代码解释器沙箱。
*   **考核要求：**
    1.  **Prompt 强约束：** 修改 Qwen 的 System Prompt，禁止模型直接输出计算结果。必须将自然语言问题（如“计算 $\int_{0}^{1} x^2 dx$ 的结果”）转化为标准的 Python 代码块（`print` 输出结果）。
    2.  **执行与反馈闭环：** 编写 Python 脚本自动提取 Qwen 生成的代码并在沙箱运行。如果代码报错（SyntaxError），需将错误信息回传给 Qwen 进行 **Self-Correction（自我修正）**，直到代码成功运行。
    3.  **100% 准确率验收：** 必须准确输出浮点数结果。测试集包含复杂的单位换算（如“50英里/小时等于多少米/秒”），要求 Agent 必须调用 Python 库进行换算，**严禁使用模型内部知识估算**。

**题目 2：逻辑谜题终结者 (Symbolic Solver)**
*   **项目背景：** 解决经典的“狼羊菜过河”或“数独”问题。依靠 LLM 直接推理很难保证步骤完全合法（如第 5 步突然违规）。
*   **基础环境：** **Qwen-7B-Instruct**，一个定义了游戏规则验证器（Validator）的 Python 类。
*   **考核要求：**
    1.  **代码生成而非文本推理：** 要求 Qwen-7B 不再生成“第一步带羊过河”的文本，而是生成一段 **BFS/DFS 搜索算法的 Python 代码** 或调用预定义的 `Environment` 类的方法。
    2.  **状态校验回路：** 实现一个 `While` 循环。每当 Qwen 生成一步操作，立即调用 `Validator` 检查合法性。
        *   如果非法（如“狼吃羊”），程序自动拦截并返回错误提示：“Action Illegal: Wolf eats Sheep”，强制 Qwen 重新生成该步骤。
    3.  **验收标准：** 在 100 次随机初始化的测试中，Agent 必须 **100% 通关**，且生成的步骤序列中不能包含任何一步非法操作（非法操作必须被代码逻辑拦截在输出之前）。

**题目 3：结构化信息零遗漏提取 (Json Mode)**
*   **项目背景：** 从复杂的案件卷宗（长文本）中提取“嫌疑人、时间、地点、证据”四元组。
*   **基础环境：** **Qwen-7B**（具备强大的指令跟随能力），包含干扰信息的长文本数据集。
*   **考核要求：**
    1.  **强制结构化输出：** 利用 Qwen 的 Function Calling 或 JSON Mode，强制模型仅输出 JSON 格式。
    2.  **双重验证机制 (Dual-Verification)：**
        *   **Round 1：** 让 Qwen 提取所有线索。
        *   **Round 2：** 编写代码，将提取出的 JSON 字段作为 Query，反向让 Qwen 在原文中寻找“原文证据片段（Quote）”。若找不到原文支撑，自动过滤该条目（消除幻觉）。
    3.  **验收标准：** 提取结果的 F1-Score 需达到 0.99 以上。**关键实体（人名、时间）准确率必须为 100%**，不能有多字、少字或错别字。

---

### **第二方向：深度学习 (CV 与多模态)**
> **考核目标：** 视觉感知、语义理解与多模态融合。

**题目 4：构建以图搜图系统 (CLIP)**
*   **对应考点：** 经典 CV 算法、图文检索
*   **任务描述：** 基于预训练的 CLIP (ViT-B/32) 模型，实现一个电商商品检索引擎。
*   **考核要求：**
    1.  编写 ETL 脚本：读取 images 文件夹，批量推理生成 Image Embeddings 并存入向量库（FAISS）。
    2.  实现 `text_to_image_search(query)`：将用户文本（如“碎花连衣裙”）转为 Text Embedding 并进行 Top-K 检索。
    3.  **优化：** 对比仅使用文本 `label` 与使用 Prompt 模版 `"A photo of a {label}, high quality product"` 时的检索准确率（Recall@5）。

**题目 5：BERT 意图识别与难例挖掘**
*   **对应考点：** 语言理解模型、语义匹配
*   **任务描述：** 优化一个用于客服系统的 BERT 分类模型。
*   **考核要求：**
    1.  在 `train.py` 中自定义 **Focal Loss**，解决训练数据中“投诉类”样本过少导致的类别不平衡问题。
    2.  实现“难例挖掘（Hard Negative Mining）”：在验证集上找出置信度低或预测错误的样本，并编写脚本自动将其加入下一轮微调队列。
    3.  导出模型为 ONNX 格式，并验证推理精度无损失。

**题目 6：VLM 视觉迷宫导航**
*   **对应考点：** 多模态融合、视觉迷宫任务
*   **任务描述：** 给定一张包含起点 S 和终点 E 的迷宫图片，要求 VLM 输出路径。
*   **考核要求：**
    1.  调用 LLaVA 或 Qwen-VL 接口，设计 System Prompt，要求模型输出 `U(上), D(下), L(左), R(右)` 的指令序列。
    2.  **图像增强处理：** 编写 OpenCV 代码，在原始迷宫图上叠加 $10 \times 10$ 的网格线和坐标标号，以辅助 VLM 定位。
    3.  **验证：** 比较叠加网格线前后，模型输出路径的可行性（即是否撞墙）。

---

### **第三方向：AI Infra (大模型推理优化)**
> **考核目标：** 高并发、低延迟的系统级优化。

**题目 7：KV Cache 管理与显存优化**
*   **对应考点：** KV Cache 管理、长上下文复用
*   **任务描述：** 模拟一个 Transformer 推理循环，解决显存爆炸问题。
*   **考核要求：**
    1.  在 Attention 层实现 `K_Cache` 和 `V_Cache` 的动态拼接逻辑，避免对历史 Token 重复计算。
    2.  实现 **PagedAttention** 的简化版：申请非连续的显存块（Block）来存储 KV Cache，并通过 Block Table 进行索引。
    3.  **指标：** 在 Context Length = 4096 时，相比无 Cache 版本，首字延迟（TTFT）需降低 50% 以上。

**题目 8：动态批处理 (Continuous Batching)**
*   **对应考点：** 动态批处理、GPU 利用率提升
*   **任务描述：** 传统 Batching 需等待最长请求结束，导致 GPU 空转。
*   **考核要求：**
    1.  重构 Inference Loop：在每个 Decoding Step 结束后，检测是否有请求已生成 EOS（结束符）。
    2.  实现调度策略：一旦检测到某请求结束，立即移除该请求，并从等待队列（Waiting Queue）中插入新请求填补空位。
    3.  **输出：** 打印时间轴日志，证明新请求是在旧请求尚未全部结束时插入执行的。

**题目 9：Triton 算子融合 (Operator Fusion)**
*   **对应考点：** 算子融合、吞吐量提升
*   **任务描述：** 模型分析发现 `RMSNorm` 和 `SiLU` 层频繁读写内存，需进行 Kernel 融合。
*   **考核要求：**
    1.  使用 OpenAI Triton 语言编写一个融合算子 Kernel：`FusedRMSNormSiLU`。
    2.  在 PyTorch 中封装该 Kernel 为 `torch.autograd.Function`。
    3.  **Benchmark：** 在输入维度 `[Batch, Seq, Hidden]` 为 `[32, 1024, 4096]` 下，对比原生 PyTorch 实现与 Triton 实现的执行耗时。

---

### **第四方向：强化学习 (决策优化)**
> **考核目标：** 策略学习、奖励设计与稳定性优化。

**题目 10：PPO 算法核心实现**
*   **对应考点：** PPO 算法、任务目标完善
*   **任务描述：** 在 `LunarLander-v2` 环境中训练智能体。
*   **考核要求：**
    1.  补全 `PPO_Agent` 类中的 `compute_gae` 函数，实现广义优势估计（GAE）。
    2.  实现 `clip_loss`：$L^{CLIP} = \min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)$。
    3.  **调试：** 调整 `entropy_coef`（熵系数），在 TensorBoard 中展示它如何影响 Agent 的探索能力（Entropy 曲线变化）。

**题目 11：奖励函数设计 (Reward Shaping)**
*   **对应考点：** 奖励函数设计、避免策略退化
*   **任务描述：** 机器人为了得分，在原地反复“拿起-放下”货物。
*   **考核要求：**
    1.  修改 Reward 函数：增加状态标志位，规定“同一货物被拿起后必须运送到目的地才能再次得分”。
    2.  引入 **势能奖励 (Potential-based Reward)**：$R = \gamma \Phi(S') - \Phi(S)$，其中 $\Phi(S)$ 为当前位置到目标的负距离。
    3.  **验证：** 训练曲线应显示 Agent 收敛速度加快，且不再出现原地刷分行为。

**题目 12：GRPO (组相对策略优化)**
*   **对应考点：** GRPO、DeepSeek-R1 相关技术
*   **任务描述：** 针对数学推理任务，使用 GRPO 替代传统的 PPO Critic 架构。
*   **考核要求：**
    1.  实现 **Group Sampling**：对于同一个问题 $q$，让旧策略 $\pi_{\theta_{old}}$ 采样生成 $G=8$ 个不同的答案 $\{o_1, ..., o_G\}$。
    2.  实现 **Advantage 计算**：计算这组答案的平均奖励 $\bar{r}$，则每个答案的优势 $A_i = (r_i - \bar{r}) / \sigma$。
    3.  基于计算出的 $A_i$ 更新策略模型，并打印训练过程中 KL 散度的变化。

---

### **第五方向：RAG (检索增强生成)**
**核心要求：** 使用 **Qwen-7B** 作为核心生成与校验模型；消除“幻觉”，确保回答完全忠实于检索内容（Faithfulness = 100%）。

**题目 13：高精度 Query 理解与意图路由**
*   **项目背景：** 用户提问往往模糊。为了保证检索的绝对精准，需要对 Query 进行精准改写或路由。
*   **基础环境：** **Qwen-7B**，Elasticsearch/ChromaDB。
*   **考核要求：**
    1.  **关键词精准提取：** 利用 Qwen-7B 将用户口语化表达（如“那个红色的水果手机”）转化为精确的数据库查询条件（`{"category": "phone", "brand": "Apple", "color": "red"}`）。要求构建 Few-shot Prompt 确保提取的 JSON 字段 **100% 符合 Schema 定义**。
    2.  **意图识别路由：** 训练/微调 Qwen 或设计 Prompt，使其能以 **100% 的准确率** 判断用户意图是“查数据”、“闲聊”还是“写代码”，并分流到不同处理链路。
    3.  **验收标准：** 在测试集上，SQL/DSL 生成的语法错误率为 0%。

**题目 14：基于 Qwen 的重排序与噪声清洗 (Listwise Rerank)**
*   **项目背景：** 向量检索召回的 Top-50 文档中包含大量无关噪声，直接输入会导致回答准确率下降。
*   **基础环境：** **Qwen-7B**（作为判别器使用）。
*   **考核要求：**
    1.  **Listwise Reranking：** 编写代码，将召回的 10 个文档一次性输入给 Qwen-7B，要求其输出一个排序后的 ID 列表 `[3, 1, 5, ...]`，并剔除无关文档。
    2.  **思维链过滤 (CoT Filtering)：** 对每一篇保留的文档，要求 Qwen 输出一句理由：“保留文档 A，因为它包含了 2023 年的财报数据”。
    3.  **验收标准：** 最终输入给生成模型的 Context 中，**噪声文档（不相关文档）的比例需降至 0%**，确保生成的答案不受干扰信息误导。

**题目 15：Verifiable RAG (可验证的检索生成)**
*   **项目背景：** 医疗或金融领域要求回答必须有据可查，严禁编造。
*   **基础环境：** **Qwen-7B**，带有 `source_id` 的切片文档库。
*   **考核要求：**
    1.  **原句引用约束：** 设计 System Prompt，强制 Qwen-7B 在回答时，必须**逐字引用**原文片段作为依据，格式为 `答案内容 (引用: "原文片段") [Source ID]`。
    2.  **后处理校验器 (Post-hoc Verifier)：** 编写一个 Python 函数，使用字符串匹配算法（如 Fuzzy Matching 阈值 > 95%）验证模型生成的“引用内容”是否真实存在于 Context 中。
        *   **逻辑闭环：** 如果校验失败（发现幻觉），自动触发 **Retry 机制**，将“请勿编造，必须引用原文”作为新的 Prompt 再次请求 Qwen，最多重试 3 次。
    3.  **验收标准：** 最终输出的答案中，**幻觉率（Hallucination Rate）必须为 0%**。所有事实性陈述必须 100% 对应原文出处。